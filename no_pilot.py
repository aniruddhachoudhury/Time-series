# -*- coding: utf-8 -*-
"""no pilot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z1lo9IfSSYrP4mPrzUZArdUuYkKfWtML
"""

from google.colab import files
files.upload()

result_nopilot = pd.read_csv('result_nopilot1.csv')
result_nopilot['Mnemonic Code'] = result_nopilot['Mnemonic Code'].replace({'VRB':'MIAVB', 'EPC':'MIAEP','ANG':'MIAAG','EVT':'NYCAA',
      'MSE':'NTCME','LAP':'LAXWE','HWL':'LAXWM','TMO':'SFOBC'})
result_nopilot.info()
result_nopilot['Day of Confirmation Date'] = pd.to_datetime(result_nopilot['Day of Confirmation Date'], errors='coerce')

result_nopilot=result_nopilot.sort_values(['Day of Confirmation Date'])
result_nopilot['month_year'] = result_nopilot['Day of Confirmation Date'].dt.to_period('M') 
result_nopilot['Total_Bookings']=result_nopilot.groupby(['Mnemonic Code','month_year','Booking Status'])['Bookings'].transform('sum')
result_nopilot.info()

Forecast1=result_nopilot.drop(['Day of Confirmation Date',
       'Bookings','Week_Number','year'], axis=1)
Forecast1=Forecast1.drop_duplicates()


MIAVB=Forecast1[Forecast1['Mnemonic Code'].str.contains("MIAVB")]
MIAVB_Confirm=MIAVB[MIAVB['Booking Status'].str.contains("Confirmed")]
MIAVB_Confirm=MIAVB_Confirm.drop(['Mnemonic Code',
       'Booking Status'], axis=1)
MIAVB_Confirm.info()
MIAVB_Confirm.set_index('month_year', inplace=True)

MIAVB_Confirm['month'] = MIAVB_Confirm.index
MIAVB_Confirm=MIAVB_Confirm.drop(['Unnamed: 0'], axis=1)

MIAVB_Confirm=MIAVB_Confirm.drop_duplicates()

MIAVB_Confirm=MIAVB_Confirm.drop(['month'], axis=1)
MIAVB_Confirm

import numpy as np
import pandas as pd
import statsmodels.api as sm

import matplotlib.pyplot as plt
result_nopilot = pd.read_csv('result_nopilot1.csv')
result_nopilot['Mnemonic Code'] = result_nopilot['Mnemonic Code'].replace({'VRB':'MIAVB', 'EPC':'MIAEP','ANG':'MIAAG','EVT':'NYCAA',
      'MSE':'NTCME','LAP':'LAXWE','HWL':'LAXWM','TMO':'SFOBC'})
result_nopilot.info()

MIAVB_Confirm.info()

result_nopilot['Day of Confirmation Date'] = pd.to_datetime(result_nopilot['Day of Confirmation Date'], errors='coerce')

result_nopilot=result_nopilot.sort_values(['Day of Confirmation Date'])
result_nopilot['month_year'] = result_nopilot['Day of Confirmation Date'].dt.to_period('M') 
result_nopilot['Total_Bookings']=result_nopilot.groupby(['Mnemonic Code','month_year','Booking Status'])['Bookings'].transform('sum')
result_nopilot.info()

Forecast1=result_nopilot.drop(['Day of Confirmation Date',
       'Bookings','Week_Number','year'], axis=1)
Forecast1=Forecast1.drop_duplicates()

MIAVB=Forecast1[Forecast1['Mnemonic Code'].str.contains("MIAVB")]
MIAVB_Confirm=MIAVB[MIAVB['Booking Status'].str.contains("Confirmed")]
MIAVB_Confirm=MIAVB_Confirm.drop(['Mnemonic Code',
       'Booking Status'], axis=1)

MIAVB_Confirm.set_index('month_year', inplace=True)
MIAVB_Confirm.info()

MIAVB_Confirm=MIAVB_Confirm.drop(['Unnamed: 0'], axis=1)

MIAVB_Confirm.info()

MIAVB_Confirm

MIAVB_Confirm.plot(figsize=(16, 8))
plt.show()

df1=MIAVB_Confirm
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(df1['Total_Bookings'], freq=12)  
fig = plt.figure()  
fig = decomposition.plot()  
fig.set_size_inches(15, 8)

from statsmodels.tsa.stattools import adfuller

result = adfuller(df1['Total_Bookings'])

print('Augmented Dickey-Fuller Test:')
labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']

for value,label in zip(result,labels):
    print(label+' : '+str(value) )
    
if result[1] <= 0.05:
    print("strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary")
else:
    print("weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary ")

# Store in a function for later use!
def adf_check(time_series):
    """
    Pass in a time series, returns ADF report
    """
    result = adfuller(time_series)
    print('Augmented Dickey-Fuller Test:')
    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']

    for value,label in zip(result,labels):
        print(label+' : '+str(value) )
    
    if result[1] <= 0.05:
        print("strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary")
    else:
        print("weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary ")

df1['First']=df1.Total_Bookings-df1.Total_Bookings.shift(1)

adf_check(df1['Total_Bookings'].dropna())

adf_check(df1['First'].dropna())

df1['First'].plot()

#df1=df1.drop(['First'], axis=1)
df3=MIAVB_Confirm.Total_Bookings[:'2018-12']
df3=pd.DataFrame(df3)

df3

MIAVB_Confirm.tail()

# For non-seasonal data
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX

!pip install pmdarima

from statsmodels.tsa.arima_model import ARIMA
import pmdarima as pm

model = pm.auto_arima(df3.Total_Bookings, start_p=1, start_q=1,
                      test='adf',       # use adftest to find optimal 'd'
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=0,           # let model determine 'd'
                      seasonal=True,   # No Seasonality
                      start_P=0, 
                      D=0, 
                      trace=True,
                      error_action='ignore',  
                      suppress_warnings=True, 
                      stepwise=True)

print(model.summary())

df1.tail()

# We have seasonal data!
model = SARIMAX(df3.Total_Bookings,order=(2,0,0),seasonal_order=(0,0,0,1))
results = model.fit()
print(results.summary())

pred = results.get_prediction(start=pd.to_datetime('2019-01'),end=pd.to_datetime('2019-04'), dynamic=True)
pred_ci = pred.conf_int()

s_forecasted = pred.predicted_mean
s_forecasted=pd.DataFrame(s_forecasted)
s_forecasted.columns = ['pred_bookings']

s_truth = MIAVB_Confirm['2019-01':]

s_truth

df1.tail()

s_forecasted

results.resid.plot()

results.resid.plot(kind='kde')

def forecast_accuracy(forecast, actual):
    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE
    me = np.mean(forecast - actual)             # ME
    mae = np.mean(np.abs(forecast - actual))    # MAE
    mpe = np.mean((forecast - actual)/actual)   # MPE
    rmse = np.mean((forecast - actual)**2)**.5  # RMSE
    corr = np.corrcoef(forecast, actual)[0,1]   # corr
    mins = np.amin(np.hstack([forecast[:,None], 
                              actual[:,None]]), axis=1)
    maxs = np.amax(np.hstack([forecast[:,None], 
                              actual[:,None]]), axis=1)
    minmax = 1 - np.mean(mins/maxs)             # minmax
#    acf1 = acf(fc-test)[1] 
#    acf1=acf(s_forecasted['pred_bookings']-s_truth['Total_Bookings'])                     # ACF1
    return({'mape':mape, 'me':me, 'mae': mae, 
            'mpe': mpe, 'rmse':rmse, 
            'corr':corr, 'minmax':minmax})

forecast_accuracy(s_forecasted['pred_bookings'],s_truth['Total_Bookings'])

